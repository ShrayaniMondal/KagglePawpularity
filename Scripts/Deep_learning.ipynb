{"nbformat":4,"nbformat_minor":5,"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.6.8"},"colab":{"name":"Deep_learning.ipynb","provenance":[]}},"cells":[{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"b7211c23","executionInfo":{"status":"ok","timestamp":1638497826255,"user_tz":300,"elapsed":238,"user":{"displayName":"Deepsikha Das","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"04373992859565598022"}},"outputId":"d1e14761-a721-47bd-c1e6-011f46535717"},"source":["from google.colab import drive\n","drive.mount('/content/drive')\n","import sys\n","sys.path.append(\"/content/drive/My Drive/589MiniProject/\")\n","%cd /content/drive/My Drive/589MiniProject/"],"id":"b7211c23","execution_count":19,"outputs":[{"output_type":"stream","name":"stdout","text":["Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n","/content/drive/.shortcut-targets-by-id/1yyh1nEBAISidzfrj3Aql57NwD7xFpa20/589MiniProject\n"]}]},{"cell_type":"code","metadata":{"id":"943cca33","executionInfo":{"status":"ok","timestamp":1638496770105,"user_tz":300,"elapsed":17,"user":{"displayName":"Deepsikha Das","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"04373992859565598022"}}},"source":["import numpy as np\n","import torch\n","import torch.nn as nn\n","import torch.nn.functional as F\n","import torch.optim as optim\n","import torchvision\n","from torchsummary import summary\n","from torchvision import datasets, transforms, models\n","from torch.utils.data import DataLoader\n","import matplotlib.pyplot as plt\n","import os\n","import warnings\n","import pandas as pd\n","warnings.filterwarnings('ignore')"],"id":"943cca33","execution_count":5,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"25c1c564","executionInfo":{"status":"ok","timestamp":1638496725089,"user_tz":300,"elapsed":634,"user":{"displayName":"Deepsikha Das","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"04373992859565598022"}},"outputId":"60aa92a8-cf3a-4461-a6b0-2af4fa753d20"},"source":["USE_GPU = True\n","\n","dtype = torch.float32 # we will be using float throughout this tutorial\n","\n","if USE_GPU and torch.cuda.is_available():\n","    device = torch.device('cuda')\n","else:\n","    device = torch.device('cpu')\n","\n","# Constant to control how frequently we print train loss\n","print_every = 75\n","\n","print('using device:', device)"],"id":"25c1c564","execution_count":3,"outputs":[{"output_type":"stream","name":"stdout","text":["using device: cpu\n"]}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":310},"id":"eHJlmU7M_-tU","executionInfo":{"status":"ok","timestamp":1638497043076,"user_tz":300,"elapsed":357,"user":{"displayName":"Deepsikha Das","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"04373992859565598022"}},"outputId":"f70f83b9-6d88-459c-f136-4fb7f5a731b7"},"source":["train_df = pd.read_csv('Data/MetaData/train.csv')\n","test_df = pd.read_csv('Data/MetaData/test.csv')\n","val_df = pd.read_csv('Data/MetaData/valid.csv')\n","\n","train_df['img_path'] = os.getcwd() + '/Data/Train' + train_df['Id'] + '.jpg'\n","test_df['img_path'] = os.getcwd() + '/Data/Test' + test_df['Id'] + '.jpg'\n","val_df['img_path'] = os.getcwd() + '/Data/Valid' + val_df['Id'] + '.jpg'\n","test_df.head()"],"id":"eHJlmU7M_-tU","execution_count":9,"outputs":[{"output_type":"execute_result","data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>Id</th>\n","      <th>Subject Focus</th>\n","      <th>Eyes</th>\n","      <th>Face</th>\n","      <th>Near</th>\n","      <th>Action</th>\n","      <th>Accessory</th>\n","      <th>Group</th>\n","      <th>Collage</th>\n","      <th>Human</th>\n","      <th>Occlusion</th>\n","      <th>Info</th>\n","      <th>Blur</th>\n","      <th>Pawpularity</th>\n","      <th>img_path</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>dd0b6b124a9de66d3f55822b1e2f50fa</td>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>1</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>35</td>\n","      <td>/content/drive/.shortcut-targets-by-id/1yyh1nE...</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>d98e0578566c1f7c791aab11b02726e8</td>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>1</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>21</td>\n","      <td>/content/drive/.shortcut-targets-by-id/1yyh1nE...</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>00d1cb2ec8b263ae076ff95cae513a88</td>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>1</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>31</td>\n","      <td>/content/drive/.shortcut-targets-by-id/1yyh1nE...</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>facb4a4f9de4b590c95a06e3f39c4523</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>47</td>\n","      <td>/content/drive/.shortcut-targets-by-id/1yyh1nE...</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>5e159ec567ffb4a8121c235305557f96</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>25</td>\n","      <td>/content/drive/.shortcut-targets-by-id/1yyh1nE...</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["                                 Id  ...                                           img_path\n","0  dd0b6b124a9de66d3f55822b1e2f50fa  ...  /content/drive/.shortcut-targets-by-id/1yyh1nE...\n","1  d98e0578566c1f7c791aab11b02726e8  ...  /content/drive/.shortcut-targets-by-id/1yyh1nE...\n","2  00d1cb2ec8b263ae076ff95cae513a88  ...  /content/drive/.shortcut-targets-by-id/1yyh1nE...\n","3  facb4a4f9de4b590c95a06e3f39c4523  ...  /content/drive/.shortcut-targets-by-id/1yyh1nE...\n","4  5e159ec567ffb4a8121c235305557f96  ...  /content/drive/.shortcut-targets-by-id/1yyh1nE...\n","\n","[5 rows x 15 columns]"]},"metadata":{},"execution_count":9}]},{"cell_type":"code","metadata":{"id":"81511855","executionInfo":{"status":"ok","timestamp":1638497809948,"user_tz":300,"elapsed":219,"user":{"displayName":"Deepsikha Das","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"04373992859565598022"}}},"source":["data_dir = \"Data/\"\n","ITRAIN = \"Train\"\n","IVAL = \"Valid\"\n","ITEST = \"Test\"\n","# MTRAIN =\n","# MVAL =\n","# MTEST ="],"id":"81511855","execution_count":17,"outputs":[]},{"cell_type":"code","metadata":{"id":"0bd31fd6","executionInfo":{"status":"ok","timestamp":1638497850268,"user_tz":300,"elapsed":224,"user":{"displayName":"Deepsikha Das","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"04373992859565598022"}}},"source":["data_transforms = {ITRAIN : transforms.Compose([\n","    transforms.Resize(255),\n","    transforms.CenterCrop(224),\n","    transforms.ToTensor(),\n","    transforms.Normalize([0.485,0.456,0.406],[0.229,0.224,0.225])]),\n","IVAL : transforms.Compose([\n","    transforms.Resize(255),\n","    transforms.CenterCrop(224),\n","    transforms.ToTensor(),\n","    transforms.Normalize([0.485,0.456,0.406],[0.229,0.224,0.225])]),\n","ITEST : transforms.Compose([\n","    transforms.Resize(255),\n","    transforms.CenterCrop(224),\n","    transforms.ToTensor(),\n","    transforms.Normalize([0.485,0.456,0.406],[0.229,0.224,0.225])])}"],"id":"0bd31fd6","execution_count":21,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":398},"id":"56c1d1b1","executionInfo":{"status":"error","timestamp":1638497830882,"user_tz":300,"elapsed":255,"user":{"displayName":"Deepsikha Das","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"04373992859565598022"}},"outputId":"2590b33b-70cf-46e3-d19e-b14c3b3afeb4"},"source":["print(os.getcwd())\n","image_datasets = {\n","    x:datasets.ImageFolder(os.path.join(data_dir,x), transform = data_transforms[x]) for x in [ITRAIN, IVAL, ITEST]\n","}\n","data_loaders = {\n","    x:DataLoader(image_datasets[x], batch_size=64, shuffle=True, num_workers=4) for x in [ITRAIN, IVAL, ITEST]\n","}\n","print(data_loaders[ITEST])"],"id":"56c1d1b1","execution_count":20,"outputs":[{"output_type":"stream","name":"stdout","text":["/content/drive/.shortcut-targets-by-id/1yyh1nEBAISidzfrj3Aql57NwD7xFpa20/589MiniProject\n"]},{"output_type":"error","ename":"FileNotFoundError","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)","\u001b[0;32m<ipython-input-20-a7a3d4780775>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgetcwd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m image_datasets = {\n\u001b[0;32m----> 3\u001b[0;31m     \u001b[0mx\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mdatasets\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mImageFolder\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata_dir\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtransform\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdata_transforms\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mITRAIN\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mIVAL\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mITEST\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m }\n\u001b[1;32m      5\u001b[0m data_loaders = {\n","\u001b[0;32m<ipython-input-20-a7a3d4780775>\u001b[0m in \u001b[0;36m<dictcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgetcwd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m image_datasets = {\n\u001b[0;32m----> 3\u001b[0;31m     \u001b[0mx\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mdatasets\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mImageFolder\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata_dir\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtransform\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdata_transforms\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mITRAIN\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mIVAL\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mITEST\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m }\n\u001b[1;32m      5\u001b[0m data_loaders = {\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torchvision/datasets/folder.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, root, transform, target_transform, loader, is_valid_file)\u001b[0m\n\u001b[1;32m    311\u001b[0m                                           \u001b[0mtransform\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtransform\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    312\u001b[0m                                           \u001b[0mtarget_transform\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtarget_transform\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 313\u001b[0;31m                                           is_valid_file=is_valid_file)\n\u001b[0m\u001b[1;32m    314\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimgs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msamples\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torchvision/datasets/folder.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, root, loader, extensions, transform, target_transform, is_valid_file)\u001b[0m\n\u001b[1;32m    143\u001b[0m         super(DatasetFolder, self).__init__(root, transform=transform,\n\u001b[1;32m    144\u001b[0m                                             target_transform=target_transform)\n\u001b[0;32m--> 145\u001b[0;31m         \u001b[0mclasses\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mclass_to_idx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfind_classes\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mroot\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    146\u001b[0m         \u001b[0msamples\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmake_dataset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mroot\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mclass_to_idx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mextensions\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mis_valid_file\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    147\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torchvision/datasets/folder.py\u001b[0m in \u001b[0;36mfind_classes\u001b[0;34m(self, directory)\u001b[0m\n\u001b[1;32m    219\u001b[0m             \u001b[0;34m(\u001b[0m\u001b[0mTuple\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mList\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mDict\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mList\u001b[0m \u001b[0mof\u001b[0m \u001b[0mall\u001b[0m \u001b[0mclasses\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mdictionary\u001b[0m \u001b[0mmapping\u001b[0m \u001b[0meach\u001b[0m \u001b[0;32mclass\u001b[0m \u001b[0mto\u001b[0m \u001b[0man\u001b[0m \u001b[0mindex\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    220\u001b[0m         \"\"\"\n\u001b[0;32m--> 221\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mfind_classes\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdirectory\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    222\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    223\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__getitem__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindex\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mTuple\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mAny\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mAny\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torchvision/datasets/folder.py\u001b[0m in \u001b[0;36mfind_classes\u001b[0;34m(directory)\u001b[0m\n\u001b[1;32m     40\u001b[0m     \u001b[0mclasses\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msorted\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mentry\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mentry\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mscandir\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdirectory\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mentry\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_dir\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     41\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mclasses\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 42\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0mFileNotFoundError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Couldn't find any class folder in {directory}.\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     43\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     44\u001b[0m     \u001b[0mclass_to_idx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0mcls_name\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcls_name\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mclasses\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mFileNotFoundError\u001b[0m: Couldn't find any class folder in Data/Train."]}]},{"cell_type":"code","metadata":{"id":"f72db52e"},"source":["loss_history = []"],"id":"f72db52e","execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"95029f09"},"source":["def check_squared_error(loader,model):\n","    print(\"checking squared error difference\")\n","    squared_error = 0\n","    num_samples = 0\n","    y_true, y_preds = [],[]\n","    model.eval()  # set model to evaluation mode\n","    label_loader = ######\n","    with torch.no_grad():\n","        for x, y in loader:\n","            x = x.to(device=device, dtype=dtype)  # move to device, e.g. GPU\n","            y = y.to(device=device, dtype=torch.long)\n","            y_pred = model(x)\n","            squared_error += torch.sum(torch.square(y_pred - y))\n","            num_samples += preds.size(0)\n","        mse = squared_error / num_samples\n","        print('mse:',mse)\n","    return mse"],"id":"95029f09","execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"6e85f171"},"source":["def train_model(model, optimizer, epochs=1):\n","    \"\"\"\n","    Train a model on Melanoma dataset using the PyTorch Module API.\n","    \n","    Inputs:\n","    - model: A PyTorch Module giving the model to train.\n","    - optimizer: An Optimizer object we will use to train the model\n","    - epochs: (Optional) A Python integer giving the number of epochs to train for\n","    \n","    Returns: Nothing, but prints model accuracies during training.\n","    \"\"\"\n","    model = model.to(device=device)  # move the model parameters to CPU/GPU\n","    best_mse = float('inf')\n","    for e in range(epochs):\n","        print('epoch:',e)\n","        loss_history.append([]) \n","        for t, (x, y) in enumerate(data_loaders[MTRAIN]):\n","            model.train()  # put model to training mode\n","            x = x.to(device=device, dtype=dtype)  # move to device, e.g. GPU\n","            y = y.to(device=device, dtype=torch.long)\n","            img = []\n","            for id in data_loaders[MTRAIN][x][:,0]:\n","                img.append(data_loaders)\n","            scores = model(img,x)\n","            \n","            loss = F.mse_loss(scores,y)\n","            loss_history[e].append(loss.item())\n","            # print('cross-entropy loss:',loss)\n","\n","            # Zero out all of the gradients for the variables which the optimizer\n","            # will update.\n","            optimizer.zero_grad()\n","\n","            # This is the backwards pass: compute the gradient of the loss with\n","            # respect to each  parameter of the model.\n","            loss.backward()\n","\n","            # Actually update the parameters of the model using the gradients\n","            # computed by the backwards pass.\n","            optimizer.step()\n","\n","            torch.cuda.empty_cache()\n","\n","            if t % print_every == 0:\n","                print('Iteration %d, loss = %.4f' % (t, loss.item()))\n","                mse = check_squared_error(data_loaders[VAL], model)\n","                if mse > best_mse:\n","                  torch.save(model.state_dict(), \"./Models/cnn.pt\")\n","                  best_mse = mse\n","                torch.cuda.empty_cache()"],"id":"6e85f171","execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"e0e021eb"},"source":["class MyModel(nn.Module):\n","    def __init__(self):\n","        super(MyModel, self).__init__()\n","        self.cnn = models.vgg16(pretrained=False, aux_logits=False)\n","        self.cnn.fc = nn.Linear(\n","            self.cnn.fc.in_features, 4096)\n","        \n","        self.fc1 = nn.Linear(4096 + 14, 3000)\n","        self.fc2 = nn.Linear(3000, 1)\n","        \n","    def forward(self, image, meta_data):\n","        x1 = self.cnn(image)\n","        x2 = meta_data\n","        \n","        x = torch.cat((x1, x2), dim=1)\n","        x = F.relu(self.fc1(x))\n","        x = self.fc2(x)\n","        return x\n"],"id":"e0e021eb","execution_count":null,"outputs":[]}]}